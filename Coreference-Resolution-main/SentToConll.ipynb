{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 20:41:30 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 4.77MB/s]                    \n",
      "2024-05-15 20:41:31 INFO: Downloaded file to C:\\Users\\ramy6\\stanza_resources\\resources.json\n",
      "2024-05-15 20:41:31 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-05-15 20:41:31 INFO: Using device: cpu\n",
      "2024-05-15 20:41:31 INFO: Loading: tokenize\n",
      "2024-05-15 20:41:31 INFO: Loading: mwt\n",
      "2024-05-15 20:41:31 INFO: Loading: pos\n",
      "2024-05-15 20:41:32 INFO: Loading: lemma\n",
      "2024-05-15 20:41:32 INFO: Loading: constituency\n",
      "2024-05-15 20:41:32 INFO: Loading: ner\n",
      "2024-05-15 20:41:32 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"text\": \"Qais\",\n",
      "    \"lemma\": \"Qais\",\n",
      "    \"upos\": \"PROPN\",\n",
      "    \"xpos\": \"NNP\",\n",
      "    \"feats\": \"Number=Sing\",\n",
      "    \"start_char\": 0,\n",
      "    \"end_char\": 4,\n",
      "    \"ner\": \"B-PERSON\",\n",
      "    \"multi_ner\": [\n",
      "      \"B-PERSON\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"text\": \"the\",\n",
      "    \"lemma\": \"the\",\n",
      "    \"upos\": \"DET\",\n",
      "    \"xpos\": \"DT\",\n",
      "    \"feats\": \"Definite=Def|PronType=Art\",\n",
      "    \"start_char\": 5,\n",
      "    \"end_char\": 8,\n",
      "    \"ner\": \"I-PERSON\",\n",
      "    \"multi_ner\": [\n",
      "      \"I-PERSON\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"text\": \"son\",\n",
      "    \"lemma\": \"son\",\n",
      "    \"upos\": \"NOUN\",\n",
      "    \"xpos\": \"NN\",\n",
      "    \"feats\": \"Number=Sing\",\n",
      "    \"start_char\": 9,\n",
      "    \"end_char\": 12,\n",
      "    \"ner\": \"I-PERSON\",\n",
      "    \"multi_ner\": [\n",
      "      \"I-PERSON\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 4,\n",
      "    \"text\": \"of\",\n",
      "    \"lemma\": \"of\",\n",
      "    \"upos\": \"ADP\",\n",
      "    \"xpos\": \"IN\",\n",
      "    \"start_char\": 13,\n",
      "    \"end_char\": 15,\n",
      "    \"ner\": \"I-PERSON\",\n",
      "    \"multi_ner\": [\n",
      "      \"I-PERSON\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 5,\n",
      "    \"text\": \"his\",\n",
      "    \"lemma\": \"his\",\n",
      "    \"upos\": \"PRON\",\n",
      "    \"xpos\": \"PRP$\",\n",
      "    \"feats\": \"Case=Gen|Gender=Masc|Number=Sing|Person=3|Poss=Yes|PronType=Prs\",\n",
      "    \"start_char\": 16,\n",
      "    \"end_char\": 19,\n",
      "    \"ner\": \"I-PERSON\",\n",
      "    \"multi_ner\": [\n",
      "      \"I-PERSON\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 6,\n",
      "    \"text\": \"father\",\n",
      "    \"lemma\": \"father\",\n",
      "    \"upos\": \"NOUN\",\n",
      "    \"xpos\": \"NN\",\n",
      "    \"feats\": \"Number=Sing\",\n",
      "    \"start_char\": 20,\n",
      "    \"end_char\": 26,\n",
      "    \"ner\": \"E-PERSON\",\n",
      "    \"multi_ner\": [\n",
      "      \"E-PERSON\"\n",
      "    ],\n",
      "    \"misc\": \"SpaceAfter=No\"\n",
      "  }\n",
      "]: '(ROOT (S (VP (NNP Qais) (NP (NP (DT the) (NN son)) (PP (IN of) (NP (PRP$ his) (NN father)))))))'}\n",
      " (ROOT (S (VP (NNP Qais) (NP (NP (DT the) (NN son)) (PP (IN of) (NP (PRP$ his) (NN father)))))))\n",
      "['Qais', 'the', 'son', 'of', 'his', 'father']\n",
      "['(TOP(S(VP*', '(NP(NP*', '*)', '(PP*', '(NP*', '*))))))']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import stanza\n",
    "\n",
    "# Load the desired Stanza model.\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,constituency,ner,mwt')\n",
    "\n",
    "# Annotate the sentence\n",
    "# doc = nlp(\"He disdained death and was not bothered by it, challenging the provocations of the imperialist donkeys with full self-control, repeating the words 'There is no God but God and Mohammed is the messenger of God'.\")\n",
    "# doc = nlp(\"create a new sheet called 'dragon', add a column named 'china' to it\")\n",
    "doc  = nlp(\"Qais the son of his father\")\n",
    "\n",
    "contents_list = []\n",
    "brackets_list = []\n",
    "\n",
    "def replace_and_store(match):\n",
    "    content = match.group(1).split()[0]\n",
    "    bracket = match.group(1).split()[1]\n",
    "    contents_list.append(content)\n",
    "    brackets_list.append(bracket)\n",
    "\n",
    "    return '*'\n",
    "\n",
    "\n",
    "def conll_table(string, lemmas, ner, sent_count):\n",
    "    # Add newline after closed brackets without any nested brackets inside\n",
    "    \n",
    "    stared_string = re.sub(r'\\(([^()]+)\\)', replace_and_store, string)\n",
    "\n",
    "    print(brackets_list)\n",
    "    # replace 'ROOT' with 'TOP'\n",
    "    stared_string = stared_string.replace('ROOT', 'TOP')\n",
    "\n",
    "    # remove spaces from stared_string\n",
    "    stared_string = ''.join(stared_string.split())\n",
    "\n",
    "    # if '*' is followed by '*', add space between them\n",
    "    stared_string = re.sub(r'\\*(\\*)', r'* \\1', stared_string)\n",
    "    # if '**' was found add space between the '**' to be like this '* *'\n",
    "    stared_string = re.sub(r'\\*\\*', r'* *', stared_string)\n",
    "    # if '*' is followed by '(', add space between them\n",
    "    stared_string = re.sub(r'\\*(\\()', r'* \\1', stared_string)\n",
    "    # if ')' is followed by '*', add space between them\n",
    "    stared_string = re.sub(r'\\)(\\*)', r') \\1', stared_string)\n",
    "    # if ')' is followed by '(', add space between them\n",
    "    stared_string = re.sub(r'\\)(\\()', r') \\1', stared_string)\n",
    "\n",
    "    tree1 = stared_string.split()\n",
    "\n",
    "    print(tree1)\n",
    "\n",
    "    # construct a table where 1st column is the brackets and 2nd column is the content and 3rd column is the corresponding '*'\n",
    "    table = []\n",
    "    c = 0\n",
    "    for i in range(len(contents_list)):\n",
    "\n",
    "      table.append(['wb/a2e/00/a2e_0010','0',c,brackets_list[i],contents_list[i], tree1[i], lemmas[i], '-', '-', '-', ner[i],'-'])\n",
    "\n",
    "      c += 1\n",
    "      # if bracket == \".\" reset c to 0\n",
    "      if brackets_list[i] == \".\":\n",
    "        c = 0\n",
    "      \n",
    "    return table\n",
    "\n",
    "def wrapper(doc):\n",
    "    # store the constituency parse in a dictionary\n",
    "    tree_dict = {}\n",
    "    table_list = []\n",
    "    lemmas = []\n",
    "    # fill ner with _ to match the length of lemmas\n",
    "    ner = []\n",
    "\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        cons = sent.constituency\n",
    "        tree_dict[sent] = str(cons)\n",
    "        for token in sent.tokens:\n",
    "            # check if ner is 'O' then write it else add '-'\n",
    "            # print(token)\n",
    "            if token.ner == 'O':\n",
    "                ner.append('-')\n",
    "            # elif token.ner == 'S-PERSON':\n",
    "            #     # if ner is s-person then write '(PERSON)'\n",
    "            #     ner.append('(PERSON)')\n",
    "            else:\n",
    "                ner.append('-')\n",
    "\n",
    "        for word in sent.words:\n",
    "            # check if word is verb then write it else add '_'\n",
    "            if word.upos == \"VERB\":\n",
    "                lemmas.append(word.lemma)\n",
    "            else:\n",
    "                lemmas.append('-')\n",
    "            #lemmas.append(word.lemma)\n",
    "    \n",
    "    combined_tree = ''\n",
    "    for sent in doc.sentences:\n",
    "       print(tree_dict)\n",
    "       combined_tree += ' ' + ' '.join(tree_dict[sent].split())\n",
    "    print(combined_tree)\n",
    "\n",
    "    table_list.append(conll_table(combined_tree, lemmas, ner, len(doc.sentences)))\n",
    "\n",
    "    return table_list\n",
    "\n",
    "# def save_to_conll_file(table_list, filename):\n",
    "#     with open(filename, 'w') as file:\n",
    "#         file.write('#begin document (wb/a2e/00/a2e_0010); part 000')\n",
    "#         file.write('\\n')\n",
    "#         for table in table_list:\n",
    "#             for row in table:\n",
    "#                 file.write('\\t'.join(str(cell) for cell in row) + '\\n')\n",
    "#             file.write('\\n')\n",
    "#         file.write('#end document')\n",
    "\n",
    "# def save_to_conll_file(table_list, filename):\n",
    "#     with open(filename, 'w') as file:\n",
    "#         # Determine the maximum width for each column\n",
    "#         max_widths = [max(len(str(row[i])) for table in table_list for row in table) for i in range(len(table_list[0][0]))]\n",
    "        \n",
    "#         file.write('#begin document (wb/a2e/00/a2e_0010); part 000\\n')\n",
    "#         for table in table_list:\n",
    "#             for row in table:\n",
    "#                 # Align each column in a straight line\n",
    "#                 aligned_row = '\\t'.join(str(cell).ljust(width) for cell, width in zip(row, max_widths))\n",
    "#                 file.write(aligned_row + '\\n')\n",
    "#             file.write('\\n')\n",
    "#         file.write('#end document')\n",
    "\n",
    "def save_to_conll_file(table_list, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        # Determine the maximum width for each column\n",
    "        max_widths = [max(len(str(row[i])) for table in table_list for row in table) for i in range(len(table_list[0][0]))]\n",
    "        \n",
    "        file.write('#begin document (wb/a2e/00/a2e_0010); part 000\\n')\n",
    "        for table in table_list:\n",
    "            for row in table:\n",
    "                # Align each column in a straight line (right-aligned)\n",
    "                aligned_row = '    '.join(str(cell).rjust(width) for cell, width in zip(row, max_widths))\n",
    "                file.write(aligned_row + '\\n')\n",
    "            file.write('\\n')\n",
    "        file.write('#end document')\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "directory_path = \"corpus/data/conll-2012-test/data/english/annotations/wb/a2e/00/\"\n",
    "filename = directory_path + \"a2e_0010.gold_conll\"\n",
    "\n",
    "# Save the output to a .gold_conll file\n",
    "table_list = wrapper(doc)\n",
    "save_to_conll_file(table_list, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ramy6\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 5.67MB/s]                    \n",
      "2024-05-05 13:10:13 INFO: Downloaded file to C:\\Users\\ramy6\\stanza_resources\\resources.json\n",
      "2024-05-05 13:10:13 INFO: Downloading default packages for language: en (English) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/default.zip: 100%|██████████| 527M/527M [03:44<00:00, 2.35MB/s] \n",
      "2024-05-05 13:13:59 INFO: Downloaded file to C:\\Users\\ramy6\\stanza_resources\\en\\default.zip\n",
      "2024-05-05 13:14:04 INFO: Finished downloading models and saved to C:\\Users\\ramy6\\stanza_resources\n",
      "2024-05-05 13:14:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 4.33MB/s]                    \n",
      "2024-05-05 13:14:04 INFO: Downloaded file to C:\\Users\\ramy6\\stanza_resources\\resources.json\n",
      "2024-05-05 13:14:04 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-05-05 13:14:05 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-05-05 13:14:05 INFO: Using device: cpu\n",
      "2024-05-05 13:14:05 INFO: Loading: tokenize\n",
      "2024-05-05 13:14:06 INFO: Loading: mwt\n",
      "2024-05-05 13:14:06 INFO: Loading: pos\n",
      "2024-05-05 13:14:07 INFO: Loading: lemma\n",
      "2024-05-05 13:14:07 INFO: Loading: depparse\n",
      "2024-05-05 13:14:07 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sit': {'predicate': 'sit', 'arguments': [('cat', 'nsubj'), ('mat', 'obl'), ('.', 'punct')]}}\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "def get_predicate_arguments(sentence):\n",
    "    # Initialize Stanza pipeline\n",
    "    stanza.download('en')  # download English model\n",
    "    nlp = stanza.Pipeline('en', processors='tokenize,lemma,pos,depparse')\n",
    "\n",
    "    # Process the sentence\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract predicate arguments\n",
    "    predicate_arguments = {}\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.head == 0:  # ROOT word\n",
    "                predicate = word.lemma\n",
    "                predicate_arguments[predicate] = {\"predicate\": predicate, \"arguments\": []}\n",
    "                for child in sent.words:\n",
    "                    if child.head == word.id:\n",
    "                        predicate_arguments[predicate][\"arguments\"].append((child.lemma, child.deprel))\n",
    "    return predicate_arguments\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The cat sat on the mat.\"\n",
    "result = get_predicate_arguments(sentence)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det\n",
      "cat nsubj\n",
      "sat ROOT\n",
      "on prep\n",
      "the det\n",
      "mat pobj\n",
      ". punct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get dependecy tree using spacy\n",
    "import spacy\n",
    "\n",
    "def get_dependency_tree(sentence):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(sentence)\n",
    "    dependency_tree = \"\"\n",
    "    for token in doc:\n",
    "        dependency_tree += token.text + \" \" + token.dep_ + \"\\n\"\n",
    "    return dependency_tree\n",
    "\n",
    "# Example usage\n",
    "\n",
    "sentence = \"The cat sat on the mat.\"\n",
    "dependency_tree = get_dependency_tree(sentence)\n",
    "print(dependency_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     11\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe cat sat on the mat.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 12\u001b[0m conll \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_gold_conll\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(conll)\n",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m, in \u001b[0;36mcreate_gold_conll\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_gold_conll\u001b[39m(sentence):\n\u001b[1;32m----> 5\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(sentence)\n\u001b[0;32m      7\u001b[0m     conll \u001b[38;5;241m=\u001b[39m spacy_conll\u001b[38;5;241m.\u001b[39mspacy2conll(doc)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "# given a sentence, create it's gold_conll format using spacy_conll\n",
    "import spacy_conll\n",
    "import spacy\n",
    "\n",
    "def create_gold_conll(sentence):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(sentence)\n",
    "    conll = spacy_conll.spacy2conll(doc)\n",
    "    return conll\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The cat sat on the mat.\"\n",
    "conll = create_gold_conll(sentence)\n",
    "print(conll)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
