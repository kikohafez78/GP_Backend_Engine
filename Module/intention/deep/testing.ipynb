{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "from TorchCRF import CRF\n",
    "\n",
    "\n",
    "\n",
    "class BertCRFModel(nn.Module):\n",
    "    def __init__(self, config: dict, intent_labels: list[str], slot_labels: list[str], dropout: float = 0.1):\n",
    "        super(BertCRFModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.intent_labels = intent_labels\n",
    "        self.slot_labels = slot_labels\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        # Load pre-trained BERT\n",
    "        self.bert = BertModel.from_pretrained(self.config[\"model_dir\"])\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "        # Intent classification layers\n",
    "        self.intent_layer_1 = nn.Linear(self.bert.config.hidden_size, self.config[\"out_first_layer\"])\n",
    "        self.intent_activation_1 = nn.ReLU()\n",
    "        self.intent_layer_2 = nn.Linear(self.config[\"out_first_layer\"], len(self.intent_labels))\n",
    "        \n",
    "        # Slot filling layers\n",
    "        self.slot_layer_1 = nn.Linear(self.bert.config.hidden_size, self.config[\"out_first_layer\"])\n",
    "        self.slot_activation_1 = nn.ReLU()\n",
    "        self.slot_layer_2 = nn.Linear(self.config[\"out_first_layer\"], len(self.slot_labels))\n",
    "        \n",
    "        # CRF layer for slot filling\n",
    "        self.crf = CRF(len(self.slot_labels), batch_first = True)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, token_type_ids: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor = None):\n",
    "        # BERT outputs\n",
    "        bert_outputs = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        sequence_output = bert_outputs.last_hidden_state  # Sequence output for slot filling\n",
    "        pooled_output = bert_outputs.pooler_output       # Pooled output for intent classification\n",
    "        \n",
    "        # Intent classification\n",
    "        intent_output_dropout = self.dropout(pooled_output)\n",
    "        intent_out_layer_1 = self.intent_layer_1(intent_output_dropout)\n",
    "        intent_act_1 = self.intent_activation_1(intent_out_layer_1)\n",
    "        intent_logits = self.intent_layer_2(intent_act_1)\n",
    "        \n",
    "        # Slot filling\n",
    "        slot_output_dropout = self.dropout(sequence_output)\n",
    "        slot_out_layer_1 = self.slot_layer_1(slot_output_dropout)\n",
    "        slot_act_1 = self.slot_activation_1(slot_out_layer_1)\n",
    "        slot_logits = self.slot_layer_2(slot_act_1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            # CRF loss\n",
    "            loss = -self.crf(slot_logits, labels, mask=attention_mask.bool(), reduction='mean')\n",
    "            return loss, intent_logits\n",
    "        else:\n",
    "            # CRF decoding\n",
    "            slot_predictions = self.crf.decode(slot_logits, mask=attention_mask.bool())\n",
    "            return slot_predictions, intent_logits\n",
    "\n",
    "# Example usage\n",
    "config = {\n",
    "    \"model_dir\": \"bert-base-uncased\",\n",
    "    \"out_first_layer\": 768\n",
    "}\n",
    "intent_labels = [\"intent1\", \"intent2\", \"intent3\"]\n",
    "slot_labels = [\"slot1\", \"slot2\", \"slot3\", \"slot4\"]\n",
    "\n",
    "model = BertCRFModel(config, intent_labels, slot_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot Predictions (Inference): [[1, 1, 0, 0, 0, 0, 1, 0, 3, 3], [3, 1, 1, 1, 1, 1, 1, 0, 1, 1]]\n",
      "Intent Logits (Inference): tensor([[-0.3224,  0.1625, -0.1054],\n",
      "        [-0.2655,  0.1444, -0.1160]])\n",
      "Loss (Training): tensor(13.9148)\n",
      "Intent Logits (Training): tensor([[-0.3224,  0.1625, -0.1054],\n",
      "        [-0.2655,  0.1444, -0.1160]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example input configuration\n",
    "batch_size = 2\n",
    "seq_length = 10  # Adjust this as needed\n",
    "num_intent_labels = len(intent_labels)\n",
    "num_slot_labels = len(slot_labels)\n",
    "\n",
    "# Dummy input tensors\n",
    "input_ids = torch.randint(0, 30522, (batch_size, seq_length))  # Random integers representing token ids\n",
    "token_type_ids = torch.zeros((batch_size, seq_length), dtype=torch.long)  # All zeros (single sequence)\n",
    "attention_mask = torch.ones((batch_size, seq_length), dtype=torch.long)  # All ones (no padding)\n",
    "\n",
    "# Dummy labels for slot filling\n",
    "labels = torch.randint(0, num_slot_labels, (batch_size, seq_length))\n",
    "\n",
    "# Create the model instance\n",
    "model = BertCRFModel(config, intent_labels, slot_labels)\n",
    "\n",
    "# Put the model in evaluation mode (not strictly necessary for this test)\n",
    "model.eval()\n",
    "\n",
    "# Test the forward method with dummy inputs\n",
    "with torch.no_grad():\n",
    "    # Without labels (inference mode)\n",
    "    slot_predictions, intent_logits = model(input_ids, token_type_ids, attention_mask)\n",
    "    print(\"Slot Predictions (Inference):\", slot_predictions)\n",
    "    print(\"Intent Logits (Inference):\", intent_logits)\n",
    "\n",
    "    # With labels (training mode)\n",
    "    loss, intent_logits = model(input_ids, token_type_ids, attention_mask, labels)\n",
    "    print(\"Loss (Training):\", loss)\n",
    "    print(\"Intent Logits (Training):\", intent_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "d:\\conda\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "05/18/2024 19:41:52 - INFO - numexpr.utils -   Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "05/18/2024 19:41:52 - INFO - numexpr.utils -   NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# from bert_model_implementation_torch.model import BertModel\n",
    "from bert_model_implementation_torch.tokenization import  _is_control,_is_whitespace,_is_punctuation\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from keras.src.utils import to_categorical\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/Book1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>intent</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you update the sales figures in cells &lt;Cel...</td>\n",
       "      <td>Update cell range</td>\n",
       "      <td>entry and manipulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please modify the data in range &lt;Range&gt;</td>\n",
       "      <td>Update cell range</td>\n",
       "      <td>entry and manipulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I need to change the prices in cells &lt;Range&gt;</td>\n",
       "      <td>Update cell range</td>\n",
       "      <td>entry and manipulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Could you update the inventory levels from &lt;Ce...</td>\n",
       "      <td>Update cell range</td>\n",
       "      <td>entry and manipulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Please adjust the budget numbers in range &lt;Range&gt;</td>\n",
       "      <td>Update cell range</td>\n",
       "      <td>entry and manipulation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt             intent  \\\n",
       "0  Can you update the sales figures in cells <Cel...  Update cell range   \n",
       "1            Please modify the data in range <Range>  Update cell range   \n",
       "2       I need to change the prices in cells <Range>  Update cell range   \n",
       "3  Could you update the inventory levels from <Ce...  Update cell range   \n",
       "4  Please adjust the budget numbers in range <Range>  Update cell range   \n",
       "\n",
       "                  classes  \n",
       "0  entry and manipulation  \n",
       "1  entry and manipulation  \n",
       "2  entry and manipulation  \n",
       "3  entry and manipulation  \n",
       "4  entry and manipulation  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'max_len': 256,\n",
    "    'batch_size': 8,\n",
    "    'epochs': 10,\n",
    "    'lr':1e-05,\n",
    "    'out_first_layer': 768,\n",
    "    'dropout_rate': 0.1,\n",
    "    'model_dir':'bert-base-cased',\n",
    "    'ckpt_path': './ckpts',\n",
    "    'ckpt_model_path': './experiments'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_list = data.intent.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class intent_dataset:\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: BertTokenizer, max_len: int):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.classes = data.intent.unique().tolist()\n",
    "        self.y = df['intent']\n",
    "        self.x = df['prompt']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.x[index])\n",
    "        title= ''.join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt',\n",
    "            return_token_type_ids = True,\n",
    "            padding = 'max_length',\n",
    "            max_length = self.max_len,\n",
    "            truncation = True\n",
    "        )\n",
    "        # print(self.classes.index(self.y[index]))\n",
    "        target = self.classes.index(self.y[index])  # Get the class index\n",
    "        target_tensor = torch.zeros(len(self.classes),dtype= torch.float32)  # Initialize target tensor with zeros\n",
    "        target_tensor[target] = 1  # Set the corresponding index to 1\n",
    "        return {\n",
    "            'input_ids': inputs[\"input_ids\"].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'targets': target_tensor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.sample(frac=0.9, random_state=200).reset_index(drop=True)\n",
    "val = data.drop(train.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(config[\"model_dir\"])\n",
    "train_dataset = intent_dataset(train, tokenizer, config[\"max_len\"])\n",
    "val_dataset = intent_dataset(val, tokenizer, config[\"max_len\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle = True,\n",
    "    batch_size = config[\"batch_size\"],\n",
    "    num_workers = 0\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    shuffle = False,\n",
    "    batch_size = config[\"batch_size\"],\n",
    "    num_workers = 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[10]['targets']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset[0]['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckpt(ckpt_path, model, optimizer):\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    # print(ckpt)\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    # optimizer.load_state_dict(ckpt['state_dict'])\n",
    "    valid_loss_min = ckpt['valid_loss_min']\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckpt(state, is_best, ckpt_path, best_model_path):\n",
    "    f_path= ckpt_path\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_f_path = best_model_path\n",
    "        shutil.copyfile(f_path, best_f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class intent_model(nn.Module):\n",
    "    def __init__(self, config: dict, intent_labels: list[str], dropout: float = 0.1):\n",
    "        super(intent_model,self).__init__()\n",
    "        self.config = config\n",
    "        self.intent_labels = intent_labels\n",
    "        self.dropout_rate = dropout\n",
    "        self.bert = BertModel.from_pretrained(self.config[\"model_dir\"])\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        self.layer_1 = nn.Linear(self.bert.config.hidden_size,self.config[\"out_first_layer\"])\n",
    "        self.activation_1 = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(self.config[\"out_first_layer\"], len(self.intent_labels))\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, token_type_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        out_layer_1 = self.layer_1(output_dropout)\n",
    "        act_1 = self.activation_1(out_layer_1)\n",
    "        out_layer_2 = self.layer_2(act_1)\n",
    "        return out_layer_2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/16/2024 01:11:17 - INFO - bert_model_implementation_torch.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz not found in cache, downloading to C:\\Users\\kikos\\AppData\\Local\\Temp\\tmpcjv867ym\n",
      "100%|██████████| 404400730/404400730 [07:17<00:00, 925346.54B/s] \n",
      "05/16/2024 01:18:36 - INFO - bert_model_implementation_torch.file_utils -   copying C:\\Users\\kikos\\AppData\\Local\\Temp\\tmpcjv867ym to cache at C:\\Users\\kikos\\.pytorch_pretrained_bert\\a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
      "05/16/2024 01:18:36 - INFO - bert_model_implementation_torch.file_utils -   creating metadata file for C:\\Users\\kikos\\.pytorch_pretrained_bert\\a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
      "05/16/2024 01:18:36 - INFO - bert_model_implementation_torch.file_utils -   removing temp file C:\\Users\\kikos\\AppData\\Local\\Temp\\tmpcjv867ym\n",
      "05/16/2024 01:18:36 - INFO - bert_model_implementation_torch.model -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz from cache at C:\\Users\\kikos\\.pytorch_pretrained_bert\\a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
      "05/16/2024 01:18:36 - INFO - bert_model_implementation_torch.model -   extracting archive file C:\\Users\\kikos\\.pytorch_pretrained_bert\\a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c to temp dir C:\\Users\\kikos\\AppData\\Local\\Temp\\tmp0roo9uvs\n",
      "05/16/2024 01:18:39 - INFO - bert_model_implementation_torch.model -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "intent_model(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layer_1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation_1): ReLU()\n",
       "  (layer_2): Linear(in_features=768, out_features=93, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = intent_model(config, intent_list, 0.1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    targets = targets.float()\n",
    "    return nn.BCEWithLogitsLoss()(outputs, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, train_loader, val_loader, optimizer, ckpt_path, best_model_path):\n",
    "    valid_loss_min = np.Inf\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        model.train()\n",
    "        for batch_index, batch in tqdm(enumerate(train_loader)):\n",
    "            input_ids = batch['input_ids'].to(device, dtype= torch.long)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, dtype= torch.long)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device, dtype= torch.long)\n",
    "            targets = batch[\"targets\"].to(device, dtype= torch.long)\n",
    "            outputs = model(input_ids, token_type_ids, attention_mask)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += (1/(batch_index + 1)) * (loss.item() - train_loss)\n",
    "        print(f\"epoch {epoch} ended with train loss of {train_loss}\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_index, batch in tqdm(enumerate(val_loader)):\n",
    "                input_ids = batch['input_ids'].to(device, dtype= torch.long)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device, dtype= torch.long)\n",
    "                token_type_ids = batch[\"token_type_ids\"].to(device, dtype= torch.long)\n",
    "                targets = batch[\"targets\"].to(device, dtype= torch.long)\n",
    "                outputs = model(input_ids, token_type_ids, attention_mask)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                valid_loss += (1/(batch_index + 1)) * (loss.item() - valid_loss)\n",
    "        print(f\"epoch {epoch} ended with train loss of {valid_loss}\")\n",
    "        checkpoint = {\n",
    "            'epoch': epoch +1,\n",
    "            'valid_loss_min': valid_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "        save_ckpt(checkpoint, False, ckpt_path, best_model_path)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train \u001b[38;5;241m=\u001b[39m train(model, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m], train_loader, val_loader, optimizer, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt_path\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt_model_path\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[41], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, epochs, train_loader, val_loader, optimizer, ckpt_path, best_model_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     11\u001b[0m targets \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, token_type_ids, attention_mask)\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n",
      "File \u001b[1;32md:\\conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[35], line 14\u001b[0m, in \u001b[0;36mintent_model.forward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mTensor, token_type_ids: torch\u001b[38;5;241m.\u001b[39mTensor, attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m---> 14\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(input_ids, token_type_ids, attention_mask)\n\u001b[0;32m     15\u001b[0m     output_dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(output\u001b[38;5;241m.\u001b[39mpooler_output)\n\u001b[0;32m     16\u001b[0m     out_layer_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_1(output_dropout)\n",
      "File \u001b[1;32md:\\conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\GP__KRYM\\clean_gp\\Module\\intention\\deep\\bert_model_implementation_torch\\model.py:455\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[0;32m    452\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m extended_attention_mask\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;66;03m# fp16 compatibility\u001b[39;00m\n\u001b[0;32m    453\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m extended_attention_mask) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10000.0\u001b[39m\n\u001b[1;32m--> 455\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids, token_type_ids)\n\u001b[0;32m    456\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(embedding_output,\n\u001b[0;32m    457\u001b[0m                               extended_attention_mask,\n\u001b[0;32m    458\u001b[0m                               output_all_encoded_layers\u001b[38;5;241m=\u001b[39moutput_all_encoded_layers)\n\u001b[0;32m    459\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoded_layers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32md:\\conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\GP__KRYM\\clean_gp\\Module\\intention\\deep\\bert_model_implementation_torch\\model.py:141\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids)\u001b[0m\n\u001b[0;32m    138\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    140\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m words_embeddings \u001b[38;5;241m+\u001b[39m position_embeddings \u001b[38;5;241m+\u001b[39m token_type_embeddings\n\u001b[1;32m--> 141\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n\u001b[0;32m    142\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32md:\\conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\GP__KRYM\\clean_gp\\Module\\intention\\deep\\bert_model_implementation_torch\\model.py:111\u001b[0m, in \u001b[0;36mBertLayerNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 111\u001b[0m     u \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    112\u001b[0m     s \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m u)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    113\u001b[0m     x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m u) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(s \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train = train(model, config[\"epochs\"], train_loader, val_loader, optimizer, config[\"ckpt_path\"], config[\"ckpt_model_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'max_len': 256,\n",
    "    'batch_size': 8,\n",
    "    'epochs': 10,\n",
    "    'lr':1e-05,\n",
    "    'out_first_layer': 768,\n",
    "    'dropout_rate': 0.1,\n",
    "    'model_dir':'bert-base-cased',\n",
    "    'ckpt_path': './ckpts',\n",
    "    'ckpt_model_path': './experiments'\n",
    "}\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class intent_model(nn.Module):\n",
    "    def __init__(self, config: dict, intent_labels: list[str], dropout: float = 0.1):\n",
    "        super(intent_model, self).__init__()\n",
    "        self.config = config\n",
    "        self.intent_labels = intent_labels\n",
    "        self.dropout_rate = dropout\n",
    "        self.bert = BertModel.from_pretrained(self.config[\"model_dir\"])\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        self.layer_1 = nn.Linear(self.bert.config.hidden_size, self.config[\"out_first_layer\"])\n",
    "        self.activation_1 = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(self.config[\"out_first_layer\"], len(self.intent_labels))\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, token_type_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        out_layer_1 = self.layer_1(output_dropout)\n",
    "        act_1 = self.activation_1(out_layer_1)\n",
    "        out_layer_2 = self.layer_2(act_1)\n",
    "        return out_layer_2\n",
    "    \n",
    "    def predict_intent(self, text: str, tokenizer: BertTokenizer):\n",
    "        # Tokenize input text\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        input_ids = inputs['input_ids']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        # Move tensors to the appropriate device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        \n",
    "        # Make predictions\n",
    "        self.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            logits = self(input_ids, token_type_ids, attention_mask)\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Get the predicted label\n",
    "        predicted_label_idx = torch.argmax(probs, dim=1).item()\n",
    "        print(predicted_label_idx)\n",
    "        predicted_label = self.intent_labels[predicted_label_idx]\n",
    "        \n",
    "        return predicted_label, probs[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(config[\"model_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = config[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intent_model(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layer_1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation_1): ReLU()\n",
       "  (layer_2): Linear(in_features=768, out_features=93, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = intent_model(config, intent_list, 0.1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 30522. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.resize_token_embeddings(30522)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_ckpt(\"./ckpts/ckpt\", model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Delete hyperlink',\n",
       " array([0.01483624, 0.00962736, 0.01309677, 0.01109273, 0.01267296,\n",
       "        0.01391257, 0.01042866, 0.01172547, 0.01188861, 0.01360726,\n",
       "        0.01299947, 0.01062253, 0.01138435, 0.01929716, 0.01973281,\n",
       "        0.00815214, 0.01179272, 0.01155274, 0.01214933, 0.01029058,\n",
       "        0.01119961, 0.00984573, 0.01025925, 0.01101724, 0.01066194,\n",
       "        0.01142314, 0.01046023, 0.01088517, 0.01101524, 0.01014855,\n",
       "        0.01382952, 0.00946699, 0.01382294, 0.01145752, 0.01230946,\n",
       "        0.01451662, 0.0112405 , 0.0141823 , 0.00991062, 0.01725945,\n",
       "        0.01911117, 0.0094124 , 0.0120395 , 0.00818533, 0.01099228,\n",
       "        0.01047215, 0.00911422, 0.00737221, 0.01044737, 0.0114465 ,\n",
       "        0.00703791, 0.00761131, 0.00768395, 0.00895203, 0.00710685,\n",
       "        0.01233352, 0.00776994, 0.00646441, 0.00676312, 0.00951902,\n",
       "        0.01012736, 0.01270906, 0.0093776 , 0.01227192, 0.01941985,\n",
       "        0.011088  , 0.0120331 , 0.01338007, 0.00844096, 0.00996742,\n",
       "        0.00847649, 0.00942919, 0.00822799, 0.00745618, 0.00806574,\n",
       "        0.00983715, 0.00823793, 0.00884723, 0.0083127 , 0.01127029,\n",
       "        0.00678105, 0.00921604, 0.0076344 , 0.00922076, 0.00864667,\n",
       "        0.00864394, 0.00808124, 0.00893797, 0.01029067, 0.00850731,\n",
       "        0.00792284, 0.01489289, 0.00863835], dtype=float32))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_intent(\"Please modify the data in range\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "d:\\conda\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 1: 100%|██████████| 1/1 [00:06<00:00,  6.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.891477584838867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 10.42165756225586\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.549955368041992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 9.890883445739746\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.042511940002441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 9.409448623657227\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1/1 [00:00<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 9.78728199005127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.974279403686523\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 9.540262222290039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.632535934448242\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 9.254839897155762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.33405876159668\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.824129104614258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.039942741394043\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.45923137664795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 7.754698276519775\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 7.957639694213867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 7.477628707885742\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 7.7474589347839355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 7.199864864349365\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 1/1 [00:00<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 7.7237443923950195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 6.93419075012207\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 7.044919490814209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 6.681292533874512\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 7.100327491760254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 6.42641544342041\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 6.975263595581055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 6.193172454833984\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 6.572647571563721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 5.976933479309082\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 6.504485607147217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 5.777250289916992\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 6.111370086669922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 5.597453594207764\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 1/1 [00:00<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 6.043363571166992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 5.433134078979492\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 6.069851875305176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 5.280535697937012\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 5.79055118560791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 5.136474609375\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21: 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 5.767029762268066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 5.002470970153809\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22: 100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 5.585514545440674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.87251091003418\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 5.567394256591797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.75618839263916\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 5.3152594566345215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.642885684967041\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 5.15069055557251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.5320844650268555\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26: 100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 5.006831169128418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.43546199798584\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27: 100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 5.222864151000977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.347010612487793\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.840567588806152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.270637035369873\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.725435733795166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.186877250671387\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30: 100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.926697254180908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.1031293869018555\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 31: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.576796054840088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.02415132522583\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 32: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.568554878234863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.9471068382263184\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 33: 100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.522022247314453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.873081922531128\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 34: 100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.51061487197876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.7959671020507812\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.391411304473877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.728898525238037\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 36: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.37155818939209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.6616501808166504\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 37: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.415462493896484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.608078718185425\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 38: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.302008628845215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.5603151321411133\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 39: 100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.202866554260254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.5122416019439697\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.139654636383057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.4679274559020996\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 41: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.9372239112854004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.4276230335235596\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 42: 100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.9654245376586914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3925280570983887\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 43: 100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.986928701400757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.358520030975342\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 44: 100%|██████████| 1/1 [00:00<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.883028030395508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.32786226272583\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8939642906188965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.29994797706604\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 46: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.972655773162842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.276191234588623\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 47: 100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8651556968688965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.2558603286743164\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 48: 100%|██████████| 1/1 [00:00<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.9538562297821045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.2382471561431885\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 49: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.888011932373047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.222177028656006\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 50: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8464417457580566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.207167148590088\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 51: 100%|██████████| 1/1 [00:00<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.615452766418457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.1931121349334717\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 52: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.685314178466797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.179657459259033\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 53: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.5990021228790283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.167884349822998\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 54: 100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.759942054748535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.1571643352508545\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 55: 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.7579505443573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.1477248668670654\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 56: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.6756649017333984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.139347553253174\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 57: 100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.7738795280456543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.1323094367980957\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 58: 100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.644618272781372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.1267738342285156\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 59: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.670654535293579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.1228561401367188\n",
      "Saved the best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 60: 100%|██████████| 1/1 [00:00<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.5584776401519775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.120920181274414\n",
      "Saved the best model!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup, AdamW\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from TorchCRF import CRF\n",
    "\n",
    "\n",
    "class BertCRFModel(nn.Module):\n",
    "    def __init__(self, config: dict, intent_labels: list[str], slot_labels: list[str], dropout: float = 0.1):\n",
    "        super(BertCRFModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.intent_labels = intent_labels\n",
    "        self.slot_labels = slot_labels\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        # Load pre-trained BERT\n",
    "        self.bert = BertModel.from_pretrained(self.config[\"model_dir\"])\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "        # Intent classification layers\n",
    "        self.intent_layer_1 = nn.Linear(self.bert.config.hidden_size, self.config[\"out_first_layer\"])\n",
    "        self.intent_activation_1 = nn.ReLU()\n",
    "        self.intent_layer_2 = nn.Linear(self.config[\"out_first_layer\"], len(self.intent_labels))\n",
    "        \n",
    "        # Slot filling layers\n",
    "        self.slot_layer_1 = nn.Linear(self.bert.config.hidden_size, self.config[\"out_first_layer\"])\n",
    "        self.slot_activation_1 = nn.ReLU()\n",
    "        self.slot_layer_2 = nn.Linear(self.config[\"out_first_layer\"], len(self.slot_labels))\n",
    "        \n",
    "        # CRF layer for slot filling\n",
    "        self.crf = CRF(len(self.slot_labels), batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, token_type_ids: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor = None):\n",
    "        # BERT outputs\n",
    "        bert_outputs = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        sequence_output = bert_outputs.last_hidden_state  # Sequence output for slot filling\n",
    "        pooled_output = bert_outputs.pooler_output       # Pooled output for intent classification\n",
    "        \n",
    "        # Intent classification\n",
    "        intent_output_dropout = self.dropout(pooled_output)\n",
    "        intent_out_layer_1 = self.intent_layer_1(intent_output_dropout)\n",
    "        intent_act_1 = self.intent_activation_1(intent_out_layer_1)\n",
    "        intent_logits = self.intent_layer_2(intent_act_1)\n",
    "        \n",
    "        # Slot filling\n",
    "        slot_output_dropout = self.dropout(sequence_output)\n",
    "        slot_out_layer_1 = self.slot_layer_1(slot_output_dropout)\n",
    "        slot_act_1 = self.slot_activation_1(slot_out_layer_1)\n",
    "        slot_logits = self.slot_layer_2(slot_act_1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            # CRF loss\n",
    "            loss = -self.crf(slot_logits, labels, mask=attention_mask.bool(), reduction='mean')\n",
    "            return loss, intent_logits\n",
    "        else:\n",
    "            # CRF decoding\n",
    "            slot_predictions = self.crf.decode(slot_logits, mask=attention_mask.bool())\n",
    "            return slot_predictions, intent_logits\n",
    "\n",
    "# Dataset class\n",
    "class isf_dataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, intent_classes: list[str], slot_classes: list[str], tokenizer: BertTokenizer, max_len: int):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.intent_classes = intent_classes\n",
    "        self.slot_classes = slot_classes\n",
    "        self.intents = df['intent']\n",
    "        self.slots = df[\"slots\"]\n",
    "        self.x = df['prompt']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.x[index])\n",
    "        title = ''.join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            truncation=True\n",
    "        )\n",
    "        intent_target = self.intent_classes.index(self.intents[index])\n",
    "        slot_targets = [self.slot_classes.index(slot) for slot in list(self.slots[index])][:self.max_len] + [self.slot_classes.index(\"O\")] * (self.max_len - len(list(self.slots[index])))\n",
    "        return {\n",
    "            'input_ids': inputs[\"input_ids\"].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'intent_targets': torch.tensor(intent_target, dtype=torch.long),\n",
    "            'slot_targets': torch.tensor(slot_targets, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, num_epochs, device):\n",
    "    model.to(device)\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            intent_labels = batch['intent_targets'].to(device)\n",
    "            slot_labels = batch['slot_targets'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            slot_loss, intent_logits = model(input_ids, token_type_ids, attention_mask, slot_labels)\n",
    "            intent_loss = nn.CrossEntropyLoss()(intent_logits, intent_labels)\n",
    "            loss = slot_loss + intent_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Training loss: {avg_train_loss}\")\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                token_type_ids = batch['token_type_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                intent_labels = batch['intent_targets'].to(device)\n",
    "                slot_labels = batch['slot_targets'].to(device)\n",
    "\n",
    "                slot_loss, intent_logits = model(input_ids, token_type_ids, attention_mask, slot_labels)\n",
    "                intent_loss = nn.CrossEntropyLoss()(intent_logits, intent_labels)\n",
    "                loss = slot_loss + intent_loss\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f\"Validation loss: {avg_val_loss}\")\n",
    "\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(\"Saved the best model!\")\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        \"model_dir\": \"bert-base-uncased\",\n",
    "        \"out_first_layer\": 128,\n",
    "        \"dropout_rate\": 0.1,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"num_epochs\": 60,\n",
    "        \"batch_size\": 16,\n",
    "        \"max_len\": 128\n",
    "    }\n",
    "    intent_labels = [\"intent1\", \"intent2\", \"intent3\"]  # Example intents\n",
    "    slot_labels = [\"slot1\", \"slot2\", \"slot3\", \"O\"]  # Example slots including \"O\" for outside of any slot\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(config[\"model_dir\"])\n",
    "\n",
    "    # Create dummy dataframe\n",
    "    data = {\n",
    "        \"prompt\": [\"Example sentence one.\", \"Example sentence two.\"],\n",
    "        \"intent\": [\"intent1\", \"intent2\"],\n",
    "        \"slots\": [[\"O\", \"O\", \"slot1\", \"O\"], [\"O\", \"O\", \"slot2\", \"O\"]]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = isf_dataset(df, intent_labels, slot_labels, tokenizer, max_len=config[\"max_len\"])\n",
    "    val_dataset = isf_dataset(df, intent_labels, slot_labels, tokenizer, max_len=config[\"max_len\"])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # Initialize model\n",
    "    model = BertCRFModel(config, intent_labels, slot_labels)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    total_steps = len(train_loader) * config[\"num_epochs\"]\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    # Train the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = train_model(model, train_loader, val_loader, optimizer, scheduler, config[\"num_epochs\"], device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent: intent1\n",
      "Token - Slot pairs:\n",
      "[CLS]: O\n",
      "book: O\n",
      "a: O\n",
      "flight: O\n",
      "from: O\n",
      "new: O\n",
      "york: O\n",
      "to: O\n",
      "los: O\n",
      "angeles: O\n",
      "next: O\n",
      "monday: O\n",
      ".: O\n",
      "[SEP]: O\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "def predict(model, tokenizer, prompt, intent_labels, slot_labels, max_len, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        prompt,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    token_type_ids = inputs['token_type_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        slot_predictions, intent_logits = model(input_ids, token_type_ids, attention_mask)\n",
    "    \n",
    "    # Get intent prediction\n",
    "    intent_pred = torch.argmax(intent_logits, dim=1).item()\n",
    "    intent_label = intent_labels[intent_pred]\n",
    "    \n",
    "    # Get slot predictions\n",
    "    slot_predictions = slot_predictions[0]  # since batch size is 1\n",
    "    slot_labels_pred = [slot_labels[slot] for slot in slot_predictions]\n",
    "    \n",
    "    # Decode token ids to words\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    # Filter out special tokens\n",
    "    tokens = [token for token, mask in zip(tokens, attention_mask[0]) if mask == 1]\n",
    "    slot_labels_pred = slot_labels_pred[:len(tokens)]\n",
    "    \n",
    "    return intent_label, list(zip(tokens, slot_labels_pred))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming the model and tokenizer have been initialized as in the previous code\n",
    "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "    model.to(device)\n",
    "\n",
    "    # Example prompt\n",
    "    prompt = \"Book a flight from New York to Los Angeles next Monday.\"\n",
    "\n",
    "    # Predict\n",
    "    intent_label, token_slot_pairs = predict(model, tokenizer, prompt, intent_labels, slot_labels, config[\"max_len\"], device)\n",
    "\n",
    "    print(f\"Intent: {intent_label}\")\n",
    "    print(\"Token - Slot pairs:\")\n",
    "    for token, slot in token_slot_pairs:\n",
    "        print(f\"{token}: {slot}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
